import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from tqdm import tqdm

from core.dataset import *
from core.dprocessing import *

def optimize_features(features, time_steps, feature_list, dataset, initial_bound, interval, display=True):
    """
    Parameters:
    * feature_list: list of lists generated by df.values.to_list() function
    * interval: the number of sections to divide the feature list into for testing
    """
    ligands = ['CpG', 'FLA', 'FSL', 'LPS', 'P3K', 'PIC', 'R84', 'TNF']
    feature_list = feature_list[:int(len(feature_list) * initial_bound)]
    labels = dataset.iloc[:, [dataset.shape[1] - 1]] # last column of dataset = corresponding class labels
    labels = labels.to_numpy()
    labels = labels.reshape((-1, ))
    
    for i in tqdm(range(interval, 0, -1)):
        x = 1 / interval
        pfl = feature_list[:int(len(feature_list) * x)]
        data = partition_features(features, time_steps, pfl, 1, dataset) # bound = 1 because we already split the feature_list
        data = data.to_numpy()
        X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.1)
        model = xgb.XGBClassifier(use_label_encoder=False) # ignore deprecation warning
        model.fit(X_train, y_train)
        pred = model.predict(X_val)
        cr = classification_report(y_val, pred, target_names=ligands)
        
        if display is True:
            print(cr)
        
        
        
        